{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intel Tech Week workshop: Introduction to Deep Learning with Python\n",
    "# Hands-on session\n",
    "### Itay lieder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import os\n",
    "# for handeling data frames\n",
    "import pandas as pd\n",
    "\n",
    "# for numerical computations\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((X_train, y_train), (X_test, y_test)) = fashion_mnist.load_data()\n",
    "print('X_train: {}, y_train: {}, X_test: {}, y_test: {}'.format(X_train.shape,  y_train.shape, X_test.shape, y_test.shape))\n",
    "num_classes = len(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(5,5,figsize=[5,5])\n",
    "for i in range(25):\n",
    "    ax = axarr[int(i/5), int(i%5)]\n",
    "    ax.imshow(X_train[i,:], cmap='Greys')\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, [X_train.shape[0], 784])\n",
    "X_test = np.reshape(X_test, [X_test.shape[0], 784])\n",
    "print('X_train: {}, y_train: {}, X_test: {}, y_test: {}'.format(X_train.shape,  y_train.shape, X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_lists = [np.where(i == y_train)[0] for i in set(y_train)]\n",
    "concat_plot = np.vstack([X_train[idx[:200],:] for idx in idx_lists])\n",
    "plt.figure(figsize=[15,5])\n",
    "plt.imshow(concat_plot, aspect='auto', cmap='Greys')\n",
    "# plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "y_train_khot = enc.fit_transform([[i] for i in y_train]).A\n",
    "y_test_khot = enc.fit_transform([[i] for i in y_test]).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# import time\n",
    "\n",
    "# start = time.time()\n",
    "# model=xgb.XGBClassifier(objective =\"multi:softmax\", colsample_bytree = 0.8, learning_rate = 0.1,\n",
    "#                 max_depth = 6, alpha = .8, n_estimators = 10, num_class = 10, subsample = 0.8, eta = 0.08)\n",
    "# model.fit(X_train, y_train)\n",
    "# print(time.time() - start)\n",
    "\n",
    "# model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST results: accuracy = ~.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "opt = RMSprop(lr=2e-3, decay=1e-5)\n",
    "early_stop = EarlyStopping(monitor='categorical_accuracy', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "callbacks=[early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "\n",
    "# === TODO ===\n",
    "model_input = Input()\n",
    "\n",
    "# hidden layer\n",
    "hidden = Dense(128,\n",
    "                    activation='relu',\n",
    "                    name='hidden')(model_input)\n",
    "#  === TODO === fully connected layer\n",
    "\n",
    "\n",
    "# intialize model - define inputs and outputs\n",
    "model = Model(inputs=model_input, outputs=predictions)\n",
    "\n",
    "# compile model - define loss function and optimization method\n",
    "model.compile(optimizer=opt,\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up the TF session.\n",
    "K.clear_session()\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TODO ===\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the TF session.\n",
    "K.clear_session()\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TODO ===\n",
    "\n",
    "\n",
    "\n",
    "# === TODO ====\n",
    "preds = \n",
    "print(preds.shape)\n",
    "\n",
    "acc = np.equal(np.argmax(preds, 1), tf.argmax(y_test_khot, 1)).mean()\n",
    "\n",
    "# loss, acc = loaded_model.evaluate(X_test, y_test_khot, num_classes)\n",
    "print('\\nTest accuracy: {0}'.format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-cpu_training_2]",
   "language": "python",
   "name": "conda-env-tf-cpu_training_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
